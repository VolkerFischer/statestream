References
==========
([back to documentation](README.md))



* [Kingma et al. 2015]

Diederik P. Kingma, Jimmy Ba

__Adam: A Method for Stochastic Optimization__.

International Conference on Learning Representations, 2015.



* [Krueger et al. 2017]

David Krueger, Tegan Maharaj, Janos Kramar, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Aaron Courville, Christopher Pal.

__ZONEOUT: REGULARIZING RNNS BY RANDOMLY PRESERVING HIDDEN ACTIVATIONS.__

International Conference on Learning Representations, 2017.



* [Mnih et al. 2014]

Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu.

__Recurrent Models of Visual Attention__.

Advances in Neural Information Processing Systems, 2014.



* [Pereyra et al. 2017]

Gabriel Pereyra, George Tucker, Jan Chorowski, ≈Åukasz Kaiser, Geoffrey Hinton.

__Regularizing Neural Networks by Penalizing Confident Output Distributions__.

International Conference on Learning Representations, 2017.



* [RMSprop]

Divide the gradient by a running average of its recent magnitude.

http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf



* [Xavier et al. 2010]

Xavier Glorot, Yoshua Bengio.

__Understanding the difficulty of training deep feedforward neural networks__.

In Proceedings of the International Conference on Artificial Intelligence and Statistics, 2010.



* [Selvaraju et al. 2017]

Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra.

__Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization__.

[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)



* [Jaderberg et al. 2015]

Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu

__Spatial Transformer Networks__.

[https://arxiv.org/abs/1506.02025](https://arxiv.org/abs/1506.02025)
